{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/Question-Answering-Model/blob/main/Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pinecone-client cohere streamlit gradio langchain\n",
        "!pip install sentence-transformers # for generating embeddings"
      ],
      "metadata": {
        "id": "McbjbM5QS6M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "KJc_NTHTgOXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import cohere\n",
        "import os\n",
        "\n",
        "# Set your API keys here\n",
        "PINECONE_API_KEY = os.getenv('API_KEY')\n",
        "COHERE_API_KEY = os.getenv('API_KEY')"
      ],
      "metadata": {
        "id": "JRpljURiVdyS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "\n",
        "# Function to extract text from a PDF file using PyMuPDF\n",
        "def extract_text_from_pdf_with_pymupdf(pdf_file):\n",
        "    pdf_reader = fitz.open(pdf_file)\n",
        "    text = ''\n",
        "    for page_num in range(13, len(pdf_reader)):\n",
        "        page = pdf_reader.load_page(page_num)\n",
        "        text += page.get_text(\"text\")  # Extracts the text from each page\n",
        "    return text\n",
        "\n",
        "# Test with a sample PDF file\n",
        "pdf_path = '/content/ML Notes.pdf'  # Replace with your PDF path\n",
        "document_text = extract_text_from_pdf_with_pymupdf(pdf_path)\n",
        "\n",
        "print(document_text[:500].strip())  # Print the first 500 characters to verify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v4NS6r_V354",
        "outputId": "9b1d1de8-cba2-4dbd-c148-89948f48dfcd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1: What is Machine Learning?\n",
            " \n",
            "In the introduction, we briefly touched into what machine learning is\n",
            "exactly. In its essence, machine learning is a form of computer\n",
            "science technology whereby the machine itself has a complex range\n",
            "of “knowledge” that allows it to take certain data inputs and use\n",
            "complex statistical analysis strategies to create output values that fall\n",
            "within a specific range of knowledge, data, or information.\n",
            " \n",
            " \n",
            "This sounds complex, and that is because it is. However\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the extracted document text\n",
        "document_embeddings = model.encode(document_text)\n",
        "\n",
        "print(f\"Document Embeddings Shape: {document_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "jQPnBFaokKLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pinecone index\n",
        "index_name = 'qa-bot'\n",
        "\n",
        "# Initialize Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pc = Pinecone(PINECONE_API_KEY, environment='us-west1-gcp')\n",
        "\n",
        "# Check if index exists\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=len(document_embeddings), # Replace with your model dimensions\n",
        "    metric=\"cosine\", # Replace with your model metric\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    ) )\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "ry50cwgnrmKf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split the document into smaller chunks\n",
        "def split_text_into_chunks(text, chunk_size=300):\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Example: Split the document text into chunks of 300 words each\n",
        "chunks = split_text_into_chunks(document_text, chunk_size=300)\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(f\"First chunk: {chunks[1][:500]}\")  # Print first 500 characters of the first chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypP5IsV4kYoO",
        "outputId": "680abc0c-c302-4818-82b4-8ec39ad91d09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 97\n",
            "First chunk: “needs” and “learn” the upgrades accordingly. This means that the device itself would be able to create and push its own updates and become more functional based on what it was meant to do in the first place. With that being said, this definition is not always entirely accurate, as there are different types of machine learning methods. Some of them, such as the ones described previously, is completely unsupervised and should require absolutely no human intervention to be able to function and lea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to store document chunks and their embeddings in Pinecone\n",
        "def store_chunks_in_pinecone(chunks):\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_embedding = model.encode(chunk)\n",
        "\n",
        "        chunk_id = f'doc_chunk_{i}'  # Unique ID for each chunk\n",
        "        index.upsert([(chunk_id, chunk_embedding.tolist())])\n",
        "\n",
        "# Store the chunks in Pinecone\n",
        "store_chunks_in_pinecone(chunks)\n",
        "print(\"Document chunks and embeddings stored in Pinecone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDgbi7HPlGp_",
        "outputId": "a3bf53f1-dd5b-4ba9-8185-dba3afac5662"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document chunks and embeddings stored in Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate embeddings for a query\n",
        "def generate_query_embedding(query):\n",
        "    query_embedding = model.encode(query)\n",
        "    return query_embedding\n",
        "\n",
        "# Example query\n",
        "user_query_1 = \"Why Data Cleansing Is Important In Data Analysis?\"\n",
        "query_embedding = generate_query_embedding(user_query_1)\n",
        "print(f\"Query Embedding Shape: {query_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK_rZoJxudDx",
        "outputId": "df22077e-9f10-4bb4-e661-82a039fa5ff5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Embedding Shape: (384,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_documents(query_embedding, top_k=3):\n",
        "    # Ensure query embedding is a list of floats\n",
        "    query_embedding = query_embedding.astype(float).tolist()\n",
        "\n",
        "    # Search the Pinecone index for the top_k most similar document embeddings\n",
        "    result = index.query(\n",
        "       # namespace=\"example-namespace\",  # Replace with your actual namespace\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k,\n",
        "        include_values=True\n",
        "    )\n",
        "    return result"
      ],
      "metadata": {
        "id": "7btCeNwd9nQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(user_query, top_k=3):\n",
        "    # 1. Generate query embedding\n",
        "    query_embedding = generate_query_embedding(user_query)\n",
        "\n",
        "    # 2. Retrieve the most relevant chunks based on the query\n",
        "    retrieval_result = retrieve_relevant_documents(query_embedding, top_k)\n",
        "\n",
        "    # 3. Fetch the text of the top retrieved chunk\n",
        "    top_chunk_id = retrieval_result['matches'][0]['id']  # Get the top chunk ID\n",
        "    top_chunk_index = int(top_chunk_id.split('_')[-1])  # Extract chunk index from ID\n",
        "    retrieved_chunk = chunks[top_chunk_index]  # Fetch the chunk from the list of chunks\n",
        "\n",
        "    return retrieved_chunk\n",
        "\n",
        "# Example: Ask a new question and get a chunk-level answer\n",
        "new_question = \"What is Deeplearning?\"\n",
        "chunk_answer = answer_question(new_question)\n",
        "print(f\"Answer: {chunk_answer}\")  # Print first 500 characters of the chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vy9GBXgkBqA",
        "outputId": "cc25982d-5230-495a-ccb8-d5c0dc6301d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: develop a greater understanding of things like economics, demographics, and other statistics to create plans and strategies for whatever area of application they may be focusing on. Semi-Supervised Learning Method Semi-supervised is another form of a machine learning method where computers are programmed with some of the training examples missing any training labels. Still, they can be used to improve the quality of a model, allowing the device to ultimately function more effectively. Semi-supervised learning methods can range from more consistently supervised learning methods to weakly supervised learning methods. The degree to which a method is semi-supervised on this sliding scale depends on how the labels for training examples are created. In weakly supervised learning methods, the training labels are noisy, limited, or imprecise, which often helps create more effective training sets in the long run. In more strictly semi-supervised learning methods the labels are either there, or missing, there are never labels that are incorrect or sloppy. The goal of semi-supervised learning methods is to improve the outcomes of a device without making it completely unsupervised. This way, the device can be even more effective and complex, but is not entirely left to its own “devices,” so to speak. Reinforcement Learning Method The reinforcement learning method is a method that is concerned with how software agents should take action in certain environments to maximize some notion of cumulative reward. Reinforcement learning is often used in game theory, operations research, control theory, information theory, multi-agent systems, stimulation-based optimization, statistics, swarm intelligence, and genetic algorithms. For machine learning, the environment is typically represented by an “MDP” or Markov Decision Process. These algorithms do not necessarily assume knowledge, but instead are used when exact models are infeasible. In other words, they are not quite as precise or exact, but they will still\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.Client(COHERE_API_KEY)"
      ],
      "metadata": {
        "id": "mmPJAGRbr8u_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_cohere(context, question):\n",
        "    # Combine the context (retrieved chunk) and the question to form the prompt\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    # Use Cohere's generate function to generate the answer\n",
        "    response = co.generate(\n",
        "        model='command-xlarge-nightly',  # You can choose a model here\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,  # Adjust token limit as needed\n",
        "        temperature=0.7  # Adjust temperature for creativity\n",
        "    )\n",
        "\n",
        "    # Extract the generated answer\n",
        "    generated_answer = response.generations[0].text.strip()\n",
        "    return generated_answer"
      ],
      "metadata": {
        "id": "CCyPOBhjlwMA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(user_query, top_k=3):\n",
        "    # 1. Generate the query embedding\n",
        "    query_embedding = generate_query_embedding(user_query)\n",
        "\n",
        "    # 2. Retrieve the most relevant chunks from Pinecone\n",
        "    retrieval_result = retrieve_relevant_documents(query_embedding, top_k)\n",
        "    top_chunk_id = retrieval_result['matches'][0]['id']  # Get the top chunk ID\n",
        "    top_chunk_index = int(top_chunk_id.split('_')[-1])  # Extract chunk index from ID\n",
        "    retrieved_chunk = chunks[top_chunk_index]  # Fetch the chunk from the list of chunks\n",
        "\n",
        "    # 3. Generate the final answer using Cohere\n",
        "    generated_answer = generate_answer_with_cohere(retrieved_chunk, user_query)\n",
        "\n",
        "    return generated_answer\n",
        "\n",
        "# Example: Ask a new question and get a generated answer\n",
        "new_question = \"What is machine learning?\"\n",
        "final_answer = answer_question(new_question)\n",
        "print(f\"Final Answer: {final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5obmj9Aorb8",
        "outputId": "4a269113-1c7b-4178-aff2-374d5a013884"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer: Machine learning is a branch of computer science that enables machines to learn and make predictions or recommendations from data without being explicitly programmed. It involves the development of computer programs that can access data and learn from it autonomously. Machine learning devices take data as input, analyze it for patterns and specific information, and use that knowledge to make predictions or recommendations. The goal is to create systems that can learn and improve automatically, without human intervention, by recognizing patterns and making data-driven decisions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_question = \"What is Multiple Linear Regression?\"\n",
        "final_answer = answer_question(new_question)\n",
        "print(f\"Final Answer: {final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgoDplHCo_FC",
        "outputId": "9db2af35-712d-46a7-d0cb-7cd3e3be457a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer: Multiple linear regression is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. It is an extension of simple linear regression, which uses only one explanatory variable. The goal of multiple linear regression is to model the relationship between the response variable and the explanatory variables, and to estimate the coefficients of the explanatory variables that best predict the response variable.\n",
            "\n",
            "In multiple linear regression, the relationship between the response variable (Y) and the explanatory variables (X1, X2,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Function to extract text from a PDF file using PyMuPDF\n",
        "def extract_text_from_pdf_with_pymupdf(pdf_file):\n",
        "    pdf_reader = fitz.open(pdf_file)\n",
        "    text = ''\n",
        "    for page_num in range(13, len(pdf_reader)):\n",
        "        page = pdf_reader.load_page(page_num)\n",
        "        text += page.get_text(\"text\")  # Extracts the text from each page\n",
        "    return text\n",
        "# Function to handle question and answer process\n",
        "def qa_system(pdf_file, user_question):\n",
        "    # 1. Extract the document text from the uploaded PDF\n",
        "    document_text = extract_text_from_pdf_with_pymupdf(pdf_file)\n",
        "\n",
        "    # 2. Split the document into chunks, generate embeddings, and store them in Pinecone\n",
        "    chunks = split_text_into_chunks(document_text, chunk_size=300)\n",
        "    store_chunks_in_pinecone(chunks)\n",
        "\n",
        "    # 3. Retrieve and generate the answer based on the user's question\n",
        "    answer = answer_question(user_question)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Gradio interface for document upload and question answering\n",
        "def create_gradio_interface():\n",
        "    interface = gr.Interface(\n",
        "        fn=qa_system,  # The function that processes the PDF and answers the question\n",
        "        inputs=[\n",
        "            gr.components.File(label=\"Upload PDF Document\"),  # PDF file upload\n",
        "            gr.components.Textbox(label=\"Ask a question\")  # User question input\n",
        "        ],\n",
        "        outputs=gr.components.Textbox(label=\"Answer\")  # Answer output\n",
        "    )\n",
        "    return interface\n",
        "\n",
        "# Launch the Gradio app\n",
        "gradio_interface = create_gradio_interface()\n",
        "gradio_interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "4MawYIofroUv",
        "outputId": "7b4a18b4-09d8-4cb6-a5b0-164bb88cc9e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://b853c1296518a71e3f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b853c1296518a71e3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradio_interface.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F65KvPECw9Ij",
        "outputId": "ef4513a4-dbc0-4d8b-da84-e6735202699d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A9OXcacvx8D8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}